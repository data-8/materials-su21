{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "BEGIN ASSIGNMENT \n",
    "init_cell: true \n",
    "export_cell: true \n",
    "files:\n",
    "    - little_women.png\n",
    "export_cell:\n",
    "    pdf: false\n",
    "template_pdf: true\n",
    "generate:\n",
    "    points_possible: 74.0\n",
    "    pdfs:\n",
    "        course_id: 271332\n",
    "        assignment_id: 1352562\n",
    "    zips: false\n",
    "plugins:\n",
    "    - otter.plugins.builtin.GoogleSheetsGradeOverride:\n",
    "        credentials_json_path: /Users/ryanchien/data8_su21/assignments_new/data8sol-1610504852539-46c8f0024cad.json\n",
    "        sheet_url: https://docs.google.com/spreadsheets/d/18pNhcw5h4G3s_PxXyuX8oRddUcmXf2yPSQshC8gkuRI/edit#gid=0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Causality and Expressions\n",
    "\n",
    "Please complete this notebook by filling in the cells provided. Before you begin, execute the following cell to load the provided tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommended Reading:**\n",
    "- [What is Data Science](http://www.inferentialthinking.com/chapters/01/what-is-data-science.html)\n",
    "- [Causality and Experiments](http://www.inferentialthinking.com/chapters/02/causality-and-experiments.html) \n",
    "- [Programming in Python](http://www.inferentialthinking.com/chapters/03/programming-in-python.html)\n",
    "\n",
    "For all problems that you must write explanations and sentences for, you **must** provide your answer in the designated space. Moreover, throughout this homework and all future ones, please be sure to not re-assign variables throughout the notebook! For example, if you use `max_temperature` in your answer to one question, do not reassign it later on. Otherwise, you will fail tests that you thought you were passing previously!\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "This assignment is due Friday, June 25 at 11:59 P.M. Late work will not be accepted as per the [policies](http://data8.org/su21/policies.html) page.\n",
    "\n",
    "**Note: This homework has hidden tests on it. That means even though tests may say 100% passed, doesn't mean your final grade will be 100%. We will be running more tests for correctness once everyone turns in the homework.**\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged. Refer to the policies page to learn more about how to learn cooperatively.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. Office hours are held Monday-Friday. The schedule appears on [http://data8.org/su21/office-hours.html](http://data8.org/su21/office-hours.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scary Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "An ad for ADT Security Systems says,\n",
    "\n",
    "> \"When you go on vacation, burglars go to work [...] According to FBI statistics, over 25% of home burglaries occur between Memorial Day to Labor Day.\"\n",
    "\n",
    "Do the data in the ad support the claim that burglars are more likely to go to work during the time between Memorial Day to Labor Day? Please explain your answer. **(6 Points)**\n",
    "\n",
    "**Note:** You can assume that \"over 25%\" means only slightly over. Had it been much over, say closer to 30%, then the marketers would have said so.\n",
    "\n",
    "**Note:** Memorial Day is observed on the last Monday of May and Labor Day is observed on the first Monday of September.\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "manual: True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**SOLUTION:** No. Labor Day is around 14 weeks after Memorial Day, so the period between them is a bit more than 25% of the year (14/52 ~= 26%). 25% of burglaries happening in 25% of the year does not imply a higher rate of burglary in the summer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Characters in Little Women\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture, we counted the number of times that the literary characters were named in each chapter of the classic book, [*Little Women*](https://www.inferentialthinking.com/chapters/01/3/1/literary-characters). In computer science, the word \"character\" also refers to a letter, digit, space, or punctuation mark; any single element of a text. The following code generates a scatter plot in which each dot corresponds to a chapter of *Little Women*. The horizontal position of a dot measures the number of periods in the chapter. The vertical position measures the total number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell contains code that hasn't yet been covered in the course,\n",
    "# but you should be able to interpret the scatter plot it generates.\n",
    "\n",
    "from datascience import *\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "little_women_url = 'https://www.inferentialthinking.com/data/little_women.txt'\n",
    "chapters = urlopen(little_women_url).read().decode().split('CHAPTER ')[1:]\n",
    "text = Table().with_column('Chapters', chapters)\n",
    "Table().with_columns(\n",
    "    'Periods',    np.char.count(chapters, '.'),\n",
    "    'Characters', text.apply(len, 0)\n",
    "    ).scatter(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Around how many periods are there in the chapter with the most characters? Assign either 1, 2, 3, 4, or 5 to the name `characters_q1` below. **(4 Points)**\n",
    "\n",
    "1. 250\n",
    "2. 390\n",
    "3. 440\n",
    "4. 32,000\n",
    "5. 40,000\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q2_1\n",
    "manual: false\n",
    "points:\n",
    " - 0\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "characters_q1 = 2 # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "1 <= characters_q1 <= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN TEST\n",
    "characters_q1 == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test above checks that your answers are in the correct format. **This test does not check that you answered correctly**, only that you assigned a number successfully in each multiple-choice answer cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Which of the following chapters has the most characters per period? Assign either 1, 2, or 3 to the name `characters_q2` below. **(4 Points)**\n",
    "1. The chapter with about 60 periods\n",
    "2. The chapter with about 350 periods\n",
    "3. The chapter with about 440 periods\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q2_2\n",
    "manual: false\n",
    "points:\n",
    " - 0\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "characters_q2 = 1 # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "1 <= characters_q2 <= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN TEST\n",
    "characters_q2 == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the test above checks that your answers are in the correct format, but not that you have answered correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To discover more interesting facts from this plot, read [Section 1.3.2](https://www.inferentialthinking.com/chapters/01/3/2/another-kind-of-character) of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names and Assignment Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** When you run the following cell, Python produces a cryptic error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to literal (<ipython-input-8-4c8b769209ad>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-4c8b769209ad>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    4 = 2 + 2\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to literal\n"
     ]
    }
   ],
   "source": [
    "4 = 2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best explanation of what's wrong with the code, and then assign 1, 2, 3, or 4 to `names_q1` below to indicate your answer. **(4 Points)**\n",
    "\n",
    "1. Python is smart and already knows `4 = 2 + 2`.\n",
    "\n",
    "2. `4` is already a defined number, and it doesn't make sense to make a number be a name for something else. In Python, \"`x = 2 + 2`\" means \"assign `x` as the name for the value of `2 + 2`.\"\n",
    "\n",
    "3. It should be `2 + 2 = 4`.\n",
    "\n",
    "4. I don't get an error message. This is a trick question.\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q3_1\n",
    "manual: False\n",
    "points:\n",
    " - 0\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_q1 = 2 # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "1 <= names_q1 <= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN TEST\n",
    "names_q1 == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** When you run the following cell, Python will produce another cryptic error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-820d4d61e3dd>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-820d4d61e3dd>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    six = two plus two\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "two = 3\n",
    "six = two plus two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best explanation of what's wrong with the code and assign 1, 2, 3, or 4 to `names_q2` below to indicate your answer. **(4 Points)**\n",
    "\n",
    "1. The `plus` operation only applies to numbers, not the word \"two\".\n",
    "\n",
    "2. The name \"two\" cannot be assigned to the number 3.\n",
    "\n",
    "3. Two plus two is four, not six.\n",
    "\n",
    "4. Python cannot interpret the name `two` followed directly by a name that has not been defined.\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q3_2\n",
    "manual: False\n",
    "points:\n",
    " - 0\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_q2 = 4 # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "1 <= names_q2 <= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN TEST\n",
    "names_q2 == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** When you run the following cell, Python will, yet again, produce another cryptic error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-94f783b16b3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "x = print(5)\n",
    "y = x + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best explanation of what's wrong with the code and assign 1, 2, or 3 to `names_q3` below to indicate your answer. **(4 Points)**\n",
    "\n",
    "1. Python doesn't want `y` to be assigned.\n",
    "\n",
    "2. The `print` operation is meant for displaying values to the programmer, not for assigning values!\n",
    "\n",
    "3. Python can’t do addition between one name and one number. It has to be 2 numbers or 2 predefined names.\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q3_3\n",
    "manual: false\n",
    "points:\n",
    " - 0\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_q3 = 2 # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "1 <= names_q3 <= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN TEST\n",
    "names_q3 == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences between Majors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berkeley’s Office of Planning and Analysis provides data on numerous aspects of the campus. Adapted from the OPA website, the table below displays the numbers of degree recipients in three majors in the academic years 2008-2009 and 2017-2018.\n",
    "\n",
    "| Major                              | 2008-2009    | 2017-2018   |\n",
    "|------------------------------------|--------------|-------------|\n",
    "| Gender and Women's Studies         |      17      |    28       |\n",
    "| Linguistics                        |      49      |    67       |\n",
    "| Rhetoric                           |      113     |    56       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Suppose you want to find the **biggest** absolute difference between the numbers of degree recipients in the two years, among the three majors.\n",
    "\n",
    "In the cell below, compute this value and call it `biggest_change`. Use a single expression (a single line of code) to compute the answer. Let Python perform all the arithmetic (like subtracting 49 from 67) rather than simplifying the expression yourself. The built-in `abs` function takes a numerical input and returns the absolute value. The built-in `max` function can take in 3 arguments and returns the maximum of the three numbers **(5 Points)**\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q4_1\n",
    "manual: False\n",
    "points:\n",
    " - 0\n",
    " - 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biggest_change = max(abs(17 - 28), abs(49 - 67), abs(113 - 56)) # SOLUTION\n",
    "biggest_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "isinstance(biggest_change, (int, float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN TEST\n",
    "biggest_change == 57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Which of the three majors had the **smallest** absolute difference? Assign `smallest_change_major` to 1, 2, or 3 where each number corresponds to the following major:\n",
    "\n",
    "1: Gender and Women's Studies  \n",
    "2: Linguistics  \n",
    "3: Rhetoric\n",
    "\n",
    "Choose the number that corresponds to the major with the smallest absolute difference.\n",
    "\n",
    "You should be able to answer by rough mental arithmetic, without having to calculate the exact value for each major. **(4 Points)** \n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q4_2\n",
    "manual: False\n",
    "points:\n",
    " - 0\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallest_change_major = 1 # SOLUTION\n",
    "smallest_change_major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "isinstance(smallest_change_major, (int, float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN TEST\n",
    "smallest_change_major == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.**  For each major, define the “relative change” to be the following: $\\large{\\frac{\\text{absolute difference}}{\\text{value in 2008-2009}} * 100}$ \n",
    "\n",
    "Fill in the code below such that `gws_relative_change`, `linguistics_relative_change` and `rhetoric_relative_change` are assigned to the relative changes for their respective majors. **(5 Points)**\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q4_3\n",
    "manual: False\n",
    "points:\n",
    " - 0\n",
    " - 0 \n",
    " - 0\n",
    " - 1\n",
    " - 2\n",
    " - 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "for_assignment_type": "student"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64.70588235294117, 36.734693877551024, 50.442477876106196)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# BEGIN PROMPT\n",
    "gws_relative_change = (abs(...) / 17) * 100\n",
    "\"\"\"; # END PROMPT\n",
    "gws_relative_change = (abs(17 - 28) / 17) * 100 # SOLUTION NO PROMPT\n",
    "linguistics_relative_change = (abs(49 - 67) / 49) * 100 # SOLUTION\n",
    "rhetoric_relative_change = (abs(113 - 56) / 113) * 100 # SOLUTION\n",
    "gws_relative_change, linguistics_relative_change, rhetoric_relative_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "isinstance(gws_relative_change, (int, float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "isinstance(linguistics_relative_change, (int, float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "isinstance(rhetoric_relative_change, (int, float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN TEST\n",
    "gws_relative_change >= 64 and gws_relative_change <= 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN TEST\n",
    "linguistics_relative_change >= 36 and linguistics_relative_change <= 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN TEST\n",
    "rhetoric_relative_change >= 50 and rhetoric_relative_change <= 52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** Assign `biggest_rel_change_major` to 1, 2, or 3 where each number corresponds to to the following: \n",
    "\n",
    "1: Gender and Women's Studies  \n",
    "2: Linguistics  \n",
    "3: Rhetoric\n",
    "\n",
    "Choose the number that corresponds to the major with the biggest relative change. **(4 Points)**\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q4_4\n",
    "manual: False\n",
    "points:\n",
    " - 0\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign biggest_rel_change_major to the number corresponding to the major with the biggest relative change.\n",
    "biggest_rel_change_major = 1 #SOLUTION\n",
    "biggest_rel_change_major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "isinstance(biggest_rel_change_major, (int, float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN TEST\n",
    "biggest_rel_change_major == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearsightedness Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Myopia, or nearsightedness, results from a number of genetic and environmental factors. In 1999, Quinn et al studied the relation between myopia and ambient lighting at night (for example, from nightlights or room lights) during childhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**Question 1.** The data were gathered by the following procedure, reported in the study. \"Between January and June 1998, parents of children aged 2-16 years [...] that were seen as outpatients in a university pediatric ophthalmology clinic completed a questionnaire on the child's light exposure both at present and before the age of 2 years.\" Was this study observational, or was it a controlled experiment? Explain. **(5 Point)**\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q5_1\n",
    "manual: True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**SOLUTION:** It was an observational study.  The researchers didn't perform any intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**Question 2.** The study found that of the children who slept with a room light on before the age of 2, 55% were myopic. Of the children who slept with a night light on before the age of 2, 34% were myopic. Of the children who slept in the dark before the age of 2, 10% were myopic. The study concluded that, \"The prevalence of myopia [...] during childhood was strongly associated with ambient light exposure during sleep at night in the first two years after birth.\"\n",
    "\n",
    "Do the data support this statement? Why or why not? You may interpret \"strongly\" in any reasonable qualitative way. **(5 Points)**\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q5_2\n",
    "manual: True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**SOLUTION:** Yes.  There is a big difference in myopia rates between the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**Question 3.** On May 13, 1999, CNN reported the results of this study under the headline, \"Night light may lead to nearsightedness.\" Does the conclusion of the study claim that night light causes nearsightedness? **(5 Points)**\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q5_3\n",
    "manual: True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**SOLUTION:** No.  The study (as quoted above) claimed only an association."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**Question 4.** The final paragraph of the CNN report said that \"several eye specialists\" had pointed out that the study should have accounted for heredity.\n",
    "\n",
    "Myopia is passed down from parents to children. Myopic parents are more likely to have myopic children, and may also be more likely to leave lights on habitually (since the parents have poor vision). In what way does the knowledge of this possible genetic link affect how we interpret the data from the study? Explain. **(5 Points)**\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q5_4\n",
    "manual: True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**SOLUTION:** If myopic parents are more likely to have myopic kids *and* leave the lights on at night, then myopic kids are more likely to have lights on at night. It is then reasonable to assume that myopic parents are a potential confounding factor that the observational study did not account for. However, we can still find the observed association even if there is no causal effect of night lights on child myopia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studying the Survivors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reverend Henry Whitehead was skeptical of John Snow’s conclusion about the Broad Street pump. After the Broad Street cholera epidemic ended, Whitehead set about trying to prove Snow wrong.  (The history of the event is detailed [here](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1034367/pdf/medhist00183-0026.pdf).)\n",
    "\n",
    "He realized that Snow had focused his analysis almost entirely on those who had died. Whitehead, therefore, investigated the drinking habits of people in the Broad Street area who had not died in the outbreak.\n",
    "\n",
    "What is the main reason it was important to study this group? **(4 Points)**\n",
    "\n",
    "1) If Whitehead had found that many people had drunk water from the Broad Street pump and not caught cholera, that would have been evidence against Snow's hypothesis.\n",
    "\n",
    "2) Survivors could provide additional information about what else could have caused the cholera, potentially unearthing another cause.\n",
    "\n",
    "3) Through considering the survivors, Whitehead could have identified a cure for cholera.\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q6_1\n",
    "manual: False\n",
    "points:\n",
    " - 0\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Assign survivor_answer to 1, 2, or 3\n",
    "survivor_answer = 1 # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "1 <= survivor_answer <= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN TEST\n",
    "survivor_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Whitehead ended up finding further proof that the Broad Street pump played the central role in spreading the disease to the people who lived near it. Eventually, he became one of Snow’s greatest defenders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies and Administrivia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the homework is to ensure that you have read over the policies and frequently asked questions for the course. \n",
    "\n",
    "**It's important that you read through this section of the homework very carefully**. If you can get through all of this section and are sure you have all of the correct resources set up, you will be able to focus on the actual material this semester!\n",
    "\n",
    "Reading through the [policies](http://data8.org/su21/policies.html) and the [FAQ](http://data8.org/su21/faq.html) will help you get through this section very easily. It is recommended you do this before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** You have a question regarding the grading of your assignments that has not been previously answered on Piazza or the FAQ. Who do you contact? Assign `contact` to the number corresponding to the best choice below. **(4 Points)**\n",
    "\n",
    "1. The Instructors\n",
    "2. Post on Piazza\n",
    "3. Contact your Discussion TA\n",
    "4. Contact your Lab TA\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q7_1\n",
    "points:\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact = 3 # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "contact == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Why are there two assignments on Gradescope for this homework? Assign `grades` to the number corresponding to the best choice below. **(4 Points)**\n",
    "\n",
    "1. There was a mistake in the grading. I should contact someone about this\n",
    "2. One assignment is for coding questions (which I will submit to), and the other is automatically submitted for me and contains my written work.\n",
    "3. Trick question\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q7_2\n",
    "points:\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = 2 #SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "grades == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Regrade deadline dates will always be posted on the same Piazza post that releases the assignment grades, common mistakes, and solutions. Can you ask for parts of your assignment regraded after the regrade request window has passed? Assign `regrade` to the number corresponding to the best choice below. **(4 Points)**\n",
    "\n",
    "1. Yes\n",
    "2. No\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q7_3\n",
    "points:\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrade = 2 #SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "regrade == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Do you have an Gradescope account? Head to [gradescope.com](http://gradescope.com) and check if you see Data 8. If you do not, please send your Discussion TA an email with your email and student ID number. \n",
    "\n",
    "Once you have been enrolled, go to the Data 8 Gradescope course website. At the end of the url (link), you should see a number. Assign `gradescope` to that number. **(4 Points)**\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q7_4\n",
    "points:\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradescope = 271332 #SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "gradescope == 271332"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Given the following scenarios, assign `acceptable` to the scenario that is permissible given the guidelines on the [policies](http://data8.org/su21/policies.html) page. **(4 Points)**\n",
    "\n",
    "1. Alice gets stuck on a homework assignment, so she googles a fix. She stumbles across a pdf of the solutions for the homework assignment from a previous semester's offering of Data 8. After inspecting the solution, Alice writes her own solution and submits the assignment.\n",
    "\n",
    "2. After getting confused by a project, Bob asks his friend for help. His friend Emily helps by walking Bob through her own logic, without showing her code, pointing out areas that are important given the context of the question. Upon hearing his friend's logic, Bob writes his own code and completes the project.\n",
    "\n",
    "3. Eve has an extremely busy schedule, so she really wants to leave lab early by finishing it and getting checked off. Her neighbor, Charlie, simply turns his computer so Eve can see how he completed some questions. After looking at his code, Eve finishes the lab and gets checked off.\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q7_5\n",
    "points:\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable = 2 #SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "acceptable == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** To make sure you have read through the [policies](http://data8.org/su21/policies.html) and the [FAQ](http://data8.org/su21/faq.html) carefully, how many HW/lab drops are there? Assign `drops` to the number corresponding to the best choice below. **(4 Points)**\n",
    "\n",
    "1. Two homework drops and two lab drops\n",
    "2. One homework drop and one lab drop\n",
    "3. Only one homework drop\n",
    "4. One homework drop and two lab drops\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q7_6\n",
    "points:\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = 1 #SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "drops == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question 7:** Does Data 8 offer alternate exams to those with class conflicts? Assign `exams` to the number corresponding to the best choice below. **(3 Points)**\n",
    "\n",
    "1. Yes\n",
    "2. No\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q7_7\n",
    "points:\n",
    " - 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams = 1 #SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "exams == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8:** Are you actually checking Piazza? Go to this semester's [Data 8 Piazza](https://piazza.com/class/kp3pm9u0kf85n2), and find an instructor posted thread with a certain secret phrase. Assign `secret` to this secret phrase in quotes (aka as a string). **(4 Points)**\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q7_8\n",
    "points:\n",
    " - 0\n",
    " - 0\n",
    " - 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = \"motorist\" #SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "# Please actually go on Piazza and look at the threads.\n",
    "# Looks like you didn't make a string.\n",
    "type(secret) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "len(secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN TEST\n",
    "secret == \"motorist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome Survey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Once you have submitted, please also complete the welcome survey in order to receive credit for homework 1. **(1 Point)**\n",
    "\n",
    "Welcome survey is here: https://forms.gle/azz2hGwq9Zrt7NNx5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign `survey` to the secret string given at the end of the welcome survey:\n",
    "\n",
    "```\n",
    "BEGIN QUESTION\n",
    "name: q8_1\n",
    "points:\n",
    " - 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey = \"feeling datagr8\" #SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "survey == \"feeling datagr8\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
